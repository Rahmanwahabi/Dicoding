# -*- coding: utf-8 -*-
"""true

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tqRylXrDWKHiU97pgurdqxi0Yy36PplO
"""

from google.colab import drive
drive.mount('/content/drive/')

import zipfile

zip_ref = zipfile.ZipFile("/content/drive/MyDrive/bvlaja/emosi.zip", 'r')
zip_ref.extractall("/content/dataset")
zip_ref.close()

"""Importing libraries"""

>>> import nltk
  >>> nltk.download('stopwords')

"""# **Importing libraries**"""

import re
import nltk
import string
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer, WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional

#nltk.download("stopwords")
stop_words = set(stopwords.words("english"))
lemmatizer= WordNetLemmatizer()

# Modelling
from sklearn.model_selection import train_test_split,KFold, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score,confusion_matrix, classification_report
from sklearn.pipeline import Pipeline
from sklearn.metrics import f1_score
from sklearn.svm import SVC

# Read datasets
df = pd.read_csv('/content/dataset/Product Reviews Dataset for Emotions Classification Tasks - Indonesian (PRDECT-ID) Dataset/PRDECT-ID Dataset.csv')

df.head()

#print the shape of the data set
print(df.shape)

"""# **Assessing**

Train dataset
"""

#check if the data is balanced or not
df.Emotion.value_counts()

#check if the data is balanced or not
df.Emotion.value_counts() / df.shape[0] *100

plt.figure(figsize=(8,4))
sns.countplot(x='Emotion', data=df);

#print the number of null values in each column
df.isnull().sum()

#print the number of duplicated values
df.duplicated().sum()

#removing duplicated values
index = df[df.duplicated() == True].index
df.drop(index, axis = 0, inplace = True)
df.reset_index(inplace=True, drop = True)

#print the rows which are duplicated (duplicated in the Customer Review	 but with different emotions)
df[df['Customer Review'].duplicated() == True]

#print some of those rows to check
print(df[df['Customer Review'] == df.iloc[0]['Customer Review']])

#removing duplicated Customer Review
index = df[df['Customer Review'].duplicated() == True].index
df.drop(index, axis = 0, inplace = True)

#Count the number of stopwords in the data
temp =df.copy()
stop_words = set(stopwords.words("english"))
temp['stop_words'] = temp['Customer Review'].apply(lambda x: len(set(x.split()) & set(stop_words)))

print(temp['stop_words'])

#distribution of stopwords visually
temp['stop_words'].plot(kind= 'hist')

"""**Cleaning**"""

def lemmatization(text):
    lemmatizer= WordNetLemmatizer()

    text = text.split()

    text=[lemmatizer.lemmatize(y) for y in text]

    return " " .join(text)

def remove_stop_words(text):

    Text=[i for i in str(text).split() if i not in stop_words]
    return " ".join(Text)

def Removing_numbers(text):
    text=''.join([i for i in text if not i.isdigit()])
    return text

def lower_case(text):

    text = text.split()

    text=[y.lower() for y in text]

    return " " .join(text)

def Removing_punctuations(text):
    ## Remove punctuations
    text = re.sub('[%s]' % re.escape("""!"#$%&'()*+,،-./:;<=>؟?@[\]^_`{|}~"""), ' ', text)
    text = text.replace('؛',"", )

    ## remove extra whitespace
    text = re.sub('\s+', ' ', text)
    text =  " ".join(text.split())
    return text.strip()

def Removing_urls(text):
    url_pattern = re.compile(r'https?://\S+|www\.\S+')
    return url_pattern.sub(r'', text)

def remove_small_sentences(df):
    for i in range(len(df)):
        if len(df.text.iloc[i].split()) < 3:
            df.text.iloc[i] = np.nan

def normalize_text(df):
    df.Text=df.Text.apply(lambda text : lower_case(text))
    df.Text=df.Text.apply(lambda text : remove_stop_words(text))
    df.Text=df.Text.apply(lambda text : Removing_numbers(text))
    df.Text=df.Text.apply(lambda text : Removing_punctuations(text))
    df.Text=df.Text.apply(lambda text : Removing_urls(text))
    df.Text=df.Text.apply(lambda text : lemmatization(text))
    return df

def normalized_sentence(sentence):
    sentence= lower_case(sentence)
    sentence= remove_stop_words(sentence)
    sentence= Removing_numbers(sentence)
    sentence= Removing_punctuations(sentence)
    sentence= Removing_urls(sentence)
    sentence= lemmatization(sentence)
    return sentence

def normalize_text(df):
    df['Customer Review'] = df['Customer Review'].apply(lambda text: lower_case(text))
    df['Customer Review'] = df['Customer Review'].apply(lambda text: remove_stop_words(text))
    df['Customer Review'] = df['Customer Review'].apply(lambda text: Removing_numbers(text))
    df['Customer Review'] = df['Customer Review'].apply(lambda text: Removing_punctuations(text))
    df['Customer Review'] = df['Customer Review'].apply(lambda text: Removing_urls(text))
    df['Customer Review'] = df['Customer Review'].apply(lambda text: lemmatization(text))
    return df

"""**Modeling**

# **TF-IDF**
"""

#Creating a pipline using tf-idf for words embedding and different models.

#Preprocess Customer Review
X_train = df['Customer Review'].values
y_train = df['Emotion'].values

def train_model(model, data, targets):

    # Create a Pipeline object with a TfidfVectorizer and the given model
    text_clf = Pipeline([('vect',TfidfVectorizer()),
                         ('clf', model)])
    # Fit the model on the data and targets
    text_clf.fit(data, targets)
    return text_clf

def get_F1(trained_model,X,y):

    # Make predictions on the input data using the trained model
    predicted=trained_model.predict(X)
    # Calculate the F1 score for the predictions
    f1=f1_score(y,predicted, average=None)
    # Return the F1 score
    return f1

"""Training the Logistic Regression model on the Training set"""

#Train the model with the training data
log_reg = train_model(LogisticRegression(solver='liblinear',random_state = 0), X_train, y_train)

#Make a single prediction
y_pred=log_reg.predict(['Happy'])
y_pred

#test the model with the data
y_pred=log_reg.predict(X_train)

#calculate the accuracy
log_reg_accuracy = accuracy_score(y_train, y_pred)
print('Accuracy: ', log_reg_accuracy,'\n')

#calculate the F1 score
f1_Score = get_F1(log_reg,X_train,y_train)
pd.DataFrame(f1_Score, index=df.Emotion.unique(), columns=['F1 score'])

##Classification Report
print(classification_report(y_train, y_pred))

"""# **SVM**"""

#Train the model with the training data
SVM = train_model(SVC(random_state = 0), X_train, y_train)

#test the model with the test data
y_pred=SVM.predict(X_train)

#calculate the accuracy
SVM_accuracy = accuracy_score(y_train, y_pred)
print('Accuracy: ', SVM_accuracy,'\n')

#calculate the F1 score
f1_Score = get_F1(SVM,X_train,y_train)
pd.DataFrame(f1_Score, index=df.Emotion.unique(), columns=['F1 score'])

print(classification_report(y_train, y_pred))

"""# **Text Preprocessing**"""

#Splitting the Customer Review from the labels
X_train = df['Customer Review']
y_train = df['Emotion']

X_test = df['Customer Review']
y_test = df['Emotion']

X_val = df['Customer Review']
y_val = df['Emotion']

# Encode labels
le = LabelEncoder()
y_train = le.fit_transform(y_train)
y_test = le.transform(y_test)
y_val = le.transform(y_val)

#print the labels after encoding
print(set(y_train))

#Convert the class vector (integers) to binary class matrix
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)
y_val = to_categorical(y_val)

print(y_train)

"""# **Tokenizing**"""

# Tokenize words
tokenizer = Tokenizer(oov_token='UNK')
tokenizer.fit_on_texts(pd.concat([X_train, X_test], axis=0))
tokenizer.document_count

# Add 'towards' to the tokenizer's word_index with a new index
tokenizer.word_index['towards'] = len(tokenizer.word_index) + 1

#converting a single sentence to list of indexes
tokenizer.texts_to_sequences(X_train[0].split())

#convert the list of indexes into a matrix of ones and zeros (BOW)
tokenizer.texts_to_matrix(X_train[0].split())

#the sentence contains three words and the size of the vocabulary is 9012
tokenizer.texts_to_matrix(X_train[0].split()).shape

sequences_train = tokenizer.texts_to_sequences(X_train)
sequences_test = tokenizer.texts_to_sequences(X_test)
sequences_val = tokenizer.texts_to_sequences(X_val)

#print the sentence after converting them to indexes
#sequences_train

"""# **Padding**"""

df.shape

maxlen = max([len(t) for t in df['Customer Review']])
maxlen

X_train = pad_sequences(sequences_train, maxlen=229, truncating='pre')
X_test = pad_sequences(sequences_test, maxlen=229, truncating='pre')
X_val = pad_sequences(sequences_val, maxlen=229, truncating='pre')

vocabSize = len(tokenizer.index_word) + 1
print(f"Vocabulary size = {vocabSize}")

#before
sequences_train[0]

#after
X_train[0]

"""# **Model LTSM**"""

from tensorflow.keras.layers import BatchNormalization, SpatialDropout1D

model = Sequential()
model.add(Embedding(vocabSize, 200, input_length=X_train.shape[1], trainable=True))
model.add(SpatialDropout1D(0.2))
model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))
model.add(BatchNormalization())
model.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))
model.add(BatchNormalization())
model.add(Bidirectional(LSTM(32, dropout=0.2, recurrent_dropout=0.2)))
model.add(Dense(5, activation='softmax'))

model.summary()

import tensorflow as tf

tf.keras.utils.plot_model(model, show_shapes=True)

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

from keras.callbacks import EarlyStopping, ReduceLROnPlateau

class customCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if logs.get('accuracy') > 0.98 and logs.get('val_accuracy') > 0.98:
            print("\nAkurasi pada data pelatihan dan data validasi mencapai nilai > 95%")
            self.model.stop_training = True

early_stopping = EarlyStopping(
    monitor='val_accuracy',
    min_delta=0.00005,
    patience=20,
    verbose=1,
    restore_best_weights=True,
)

lr_scheduler = ReduceLROnPlateau(
    monitor='val_accuracy',
    factor=0.2,
    patience=5,
    min_lr=1e-7,
    verbose=1,
)

callbacks = [
    early_stopping,
    lr_scheduler,
    customCallback(),
]

# Fit model
history = model.fit(X_train,
                    y_train,
                    validation_data=(X_val, y_val),
                    verbose=1,
                    batch_size=256,
                    epochs=30,
                    callbacks=[callbacks]
                   )

#print the overall loss and accuracy
lstm_accuracy = model.evaluate(X_val, y_val, verbose=1)[1]

predicted = model.predict(X_test)
y_pred = predicted.argmax(axis=-1)

print(classification_report(le.transform(df['Emotion']), y_pred))

# Commented out IPython magic to ensure Python compatibility.
# Visualize Loss & Accuracy

# %matplotlib inline
import matplotlib.pyplot as plt
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'r', label='Training accuracy')
plt.plot(epochs, val_acc, 'b', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()

plt.plot(epochs, loss, 'r', label='Training Loss')
plt.plot(epochs, val_loss, 'b', label='Validation Loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

"""# **Results**"""

models = pd.DataFrame({
    'Model': ['Logistic Regression', 'SVM', 'LSTM'],
    'Accuracy': [round(log_reg_accuracy, 2), round(SVM_accuracy, 2), round(lstm_accuracy, 2)]
})

models.sort_values(by='Accuracy', ascending=False).reset_index(drop=True)

Accuracy = models.sort_values(by='Accuracy', ascending=False).reset_index(drop=True)

fig, ax = plt.subplots()
Accuracy.plot(x='Model', y='Accuracy', kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)