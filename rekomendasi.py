# -*- coding: utf-8 -*-
"""rekomendasi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NAp6bk3poWtMbq8zJ-ReDxHZ68ZmyjO4

# **Data Collection:**
"""

from google.colab import drive
drive.mount('/content/drive/')

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

import zipfile

zip_ref = zipfile.ZipFile("/content/drive/MyDrive/bvlaja/archive (4).zip", 'r')
zip_ref.extractall("/content/dataset1")
zip_ref.close()

books = pd.read_csv("/content/dataset1/Books.csv")
users = pd.read_csv("/content/dataset1/Users.csv")
rates = pd.read_csv("/content/dataset1/Ratings.csv")

"""## **Data Understanding:**"""

books

"""Dataset ini berisi informasi tentang berbagai  ISBN, judul, penulis, tahun publikasi, penerbit, URL gambar (ukuran kecil, sedang, besar)."""

books.shape

"""Dataset ini terdiri dari 271360 sample dan 8 variabel."""

users

"""Dataset ini berisi data tentang ID pengguna, lokasi, dan usia. Kode ini kemudian melakukan beberapa operasi pada dataset"""

users.shape

"""Dataset ini terdiri dari 2278858 sample dan 3 variabel."""

rates

"""Ini adalah dataset dari pemberian rating buku berdasarkan user."""

rates.shape

"""Data ini memiliki 1149780 sample dan 3 fitur."""

books.info()

users.info()

rates.info()

books.isna().sum()

books = books.dropna()

"""Menghapus missing value pada dataset books."""

books.shape

users.isna().sum()

users = users.dropna()

users.shape

rates.isna().sum()

"""Tidak ada missing value pada data rates"""

dup_books = books[books.duplicated()].shape[0]
print(f"Terdapat {dup_books} duplikat dari {books.shape[0]} sample pada books dataset.")

dup_users = users[users.duplicated()].shape[0]
print(f"Terdapat {dup_users} duplikat dari {users.shape[0]} sample pada users dataset.")

dup_rates = rates[rates.duplicated()].shape[0]
print(f"Terdapat {dup_rates} duplikat dari {rates.shape[0]} sample pada rates rates dataset.")

"""# Menghapus symbol pada judul"""

import re
def text_cleaning(text):
    text = re.sub(r'&quot;', '', text)
    text = re.sub(r'.hack//', '', text)
    text = re.sub(r'&#039;', '', text)
    text = re.sub(r'A&#039;s', '', text)
    text = re.sub(r'I&#039;', 'I\'', text)
    text = re.sub(r'&amp;', 'and', text)

    return text

books['Book-Title'] = books['Book-Title'].apply(text_cleaning)

"""## **Univariate Exploratory Data Analysis:**

*  books: berisi informasi tentang buku
*  ratings: merepresentasikan rating yang diberikan kepada buku oleh pengguna atau pembaca.
*  users: menyediakan informasi pengguna, termasuk informasi demografis.

Book Variable
"""

books['Year-Of-Publication'].astype('int')

"""Menghapus nilai dalam 'Year-Of-Publication' yang berupa teks."""

temp = (books['Year-Of-Publication'] == 'DK Publishing Inc') | (books['Year-Of-Publication'] == 'Gallimard')
books = books.drop(books[temp].index)
books[(books['Year-Of-Publication'] == 'DK Publishing Inc') | (books['Year-Of-Publication'] == 'Gallimard')]

"""Mengubah tipe data 'Year-Of-Publication'."""

books['Year-Of-Publication'] = books['Year-Of-Publication'].astype(int)
print(books.dtypes)

"""menghapus variabel yang tidak perlu dalam proses pengembangan model

*Menghapus kolom URL gambar dari semua ukuran*
"""

columns_to_drop = ['Image-URL-S', 'Image-URL-M', 'Image-URL-L']
books = books.drop(columns=columns_to_drop, errors='ignore')

books.head()

"""Ada banyak entri untuk setiap variabel."""

print("Number of Book ISBN numbers:", len(books['ISBN'].unique()))
print("Number of book titles:", len(books['Book-Title'].unique()))
print('Number of book authors:', len(books['Book-Author'].unique()))
print('Number of Publication Years:', len(books['Year-Of-Publication'].unique()))
print('Number of publisher names:', len(books['Publisher'].unique()))

"""10 penulis teratas berdasarkan jumlah buku.

*Mengelompokkan Book-Author' dan menghitung jumlah buku yang ditulis oleh setiap penulis*
"""

author_counts = books.groupby('Book-Author')['Book-Title'].count()

"""* Urutkan penulis dalam urutan menurun*"""

sorted_authors = author_counts.sort_values(ascending=False)

"""*Pilih 10 penulis teratas*"""

top_10_authors = sorted_authors.head(10)

"""*Plot 10 penulis teratas dan buku-buku yang ditulis penulis, kemudian dihitung menggunakan bar plot*"""

plt.figure(figsize=(12, 6))
top_10_authors.plot(kind='bar')
plt.xlabel('Author Name')
plt.ylabel('Number of Books')
plt.title('Top 10 Authors by Number of Books')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""Berdasarkan informasi di atas, diketahui bahwa penulis dengan nama Agatha Christie menulis buku paling banyak, yaitu lebih dari 600 buku. Dari informasi tersebut, terlihat pula bahwa dataset tersebut berisi beberapa penulis yang telah menulis lebih dari satu judul buku.

Ratings Variable
"""

rates.head()

rates.info()

print('Number of User-IDs:', len(rates['User-ID'].unique()))
print('Number of books based on ISBN:', len(rates['ISBN'].unique()))

print('Number of book ratings:')
sorted_rates = rates['Book-Rating'].value_counts().sort_index()
pd.DataFrame({'Book-Rating': sorted_rates.index, 'Sum': sorted_rates.values})

"""Berdasarkan output di atas, diketahui bahwa terdapat 105.283 pengguna yang memberikan rating buku. Jumlah buku berdasarkan ISBN yang mendapatkan rating adalah 340.556 buku, dan rating yang diberikan oleh setiap buku berkisar antara 0 hingga 10, di mana 0 adalah rating terendah dan 10 adalah rating tertinggi."""

df_rating = rates[:20000]
df_rating

"""Distribution Rating"""

book_ratings = pd.merge(books, rates, on='ISBN')
mean_rating = book_ratings.groupby('ISBN')['Book-Rating'].mean()

plt.figure(figsize=(10, 6))
plt.hist(mean_rating, bins=50)
plt.xlabel('Rata-rata Rating')
plt.ylabel('Jumlah Buku')
plt.title('Distribusi Rating Buku')
plt.show()

"""Users Variable"""

users.head()

users.info()

"""Berdasarkan informasi di atas, terdapat 278.858 entri dan 3 variabel: User-ID, yang merupakan kode unik untuk pengguna anonim; Lokasi, yang merupakan lokasi pengguna; dan Usia, yang merupakan usia pengguna. Tercatat juga bahwa ada beberapa pengguna yang usianya tidak diketahui. Data pengguna berguna ketika membuat sistem rekomendasi berdasarkan demografi atau kondisi sosial pengguna. Namun, untuk studi kasus ini, data pengguna tidak akan digunakan dalam model. Dalam pengembangan model, data yang digunakan akan berasal dari dataset "books" dan "ratings".

## **Multvarite Exploratory Data Analysis:**

*Menggabungkan DataFrame books dan rates berdasarkan ISBN*
"""

book_ratings = pd.merge(books, rates, on='ISBN')

"""1. Rating vs. Tahun Terbit:

*  Jelajahi bagaimana rating buku berubah seiring waktu.
*  Hitung rata-rata rating per tahun dan visualisasikan menggunakan plot garis.
*  Apakah terlihat ada tren (rating naik/turun seiring waktu)?

*Hitung rata-rata rating setiap tahun*
"""

rata_rata_rating_per_tahun = book_ratings.groupby('Year-Of-Publication')['Book-Rating'].mean()

"""*Plot rata-rata rating vs tahun*"""

plt.figure(figsize=(10, 6))
plt.plot(rata_rata_rating_per_tahun.index, rata_rata_rating_per_tahun.values)
plt.xlabel('Tahun Publikasi')
plt.ylabel('Rata-rata Rating')
plt.title('Rata-rata Rating Buku berdasarkan Tahun Publikasi')
plt.show()

"""2. Rating vs. Penulis:

*  Selidiki apakah ada korelasi antara penulis dan rating buku.
*  Kelompokkan rating berdasarkan penulis dan hitung rata-rata rating per penulis.
*  Visualisasikan menggunakan diagram batang.
*  Apakah ada penulis yang bukunya secara konsisten menerima rating lebih tinggi atau lebih rendah?

*Kelompokkan rating berdasarkan penulis dan hitung rata-rata rating*
"""

rata_rata_rating_per_penulis = book_ratings.groupby('Book-Author')['Book-Rating'].mean()

"""*Plot rata-rata rating vs penulis (ambil 20 teratas untuk keterbacaan)*"""

plt.figure(figsize=(15, 6))
rata_rata_rating_per_penulis.head(20).plot(kind='bar')
plt.xlabel('Nama Penulis')
plt.ylabel('Rata-rata Rating')
plt.title('Rata-rata Rating Buku berdasarkan Penulis (Top 20)')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""3. Rating vs. Jumlah Rating:

*  Analisa apakah buku dengan lebih banyak rating cenderung memiliki rata-rata rating lebih tinggi atau lebih rendah.
*  Kelompokkan rating berdasarkan ISBN dan hitung jumlah rating untuk setiap buku.
*  Gabungkan informasi ini dengan data rating asli untuk mendapatkan rating dan jumlah rating per buku.
*  Hitung rata-rata rating per buku dan visualisasikan terhadap jumlah rating menggunakan scatter plot.
*  Apakah ada pola yang terlihat?

*Hitung rating per buku (ISBN)*
"""

jumlah_rating = rates.groupby('ISBN')['Book-Rating'].count().reset_index(name='Jumlah-Rating')

"""*Gabungkan jumlah rating dengan data rating asli*"""

df_rating_gabung = pd.merge(rates, jumlah_rating, on='ISBN')

"""*Hitung rata-rata rating per buku*"""

rata_rata_rating_per_buku = df_rating_gabung.groupby('ISBN')['Book-Rating'].mean().reset_index(name='Rata-Rating')

"""*Gabungkan rata-rata rating per buku dengan jumlah rating per buku dan visualisasikan rata-rata rating vs jumlah rating menggunakan scatter plot*"""

df_rating_gabung = pd.merge(rata_rata_rating_per_buku, jumlah_rating, on='ISBN')

plt.figure(figsize=(10, 6))
plt.scatter(df_rating_gabung['Jumlah-Rating'], df_rating_gabung['Rata-Rating'], alpha=0.5)
plt.xlabel('Jumlah Rating')
plt.ylabel('Rata-rata Rating')
plt.title('Rata-rata Rating vs Jumlah Rating per Buku')
plt.show()

"""## **Data Preparation:**

**Persiapan Data untuk Model Development dengan Content-Based Filtering**


"""

all_books_clean = books.dropna()
all_books_clean

all_books_clean.isnull().sum()

"""**Standarisasi Jenis Buku Berdasarkan ISBN**

*Mengurutkan buku berdasarkan ISBN lalu memasukkannya ke dalam variabel fix_books*
"""

fix_books = all_books_clean.sort_values('ISBN', ascending=True)
fix_books

"""check the number of book titles with the following code"""

len(fix_books['ISBN'].unique())

"""Maka diperlukan proses penghapusan data duplikat di kolom 'ISBN', dan data tersebut disimpan di variabel baru bernama 'preparation'. Diterapkan di kode berikut ini."""

preparation = fix_books.drop_duplicates('ISBN')
preparation

"""Proses mengubah rangkaian data menjadi sebuah daftar dengan menggunakan fungsi tolist() dari pustaka."""

isbn_id = preparation['ISBN'].tolist()

book_title = preparation['Book-Title'].tolist()

book_author = preparation['Book-Author'].tolist()

year_of_publication = preparation['Year-Of-Publication'].tolist()

publisher = preparation['Publisher'].tolist()

print(len(isbn_id))
print(len(book_title))
print(len(book_author))
print(len(year_of_publication))
print(len(publisher))

"""Membuat dictionary untuk menentukan pasangan key-value dari data isbn_id, judul_buku, pengarang_buku, tahun_terbit, dan penerbit yang sudah disiapkan sebelumnya untuk pengembangan model sistem rekomendasi penyaringan berbasis konten."""

books_new = pd.DataFrame({
    'isbn': isbn_id,
    'book_title': book_title,
    'book_author': book_author,
    'year_of_publication': year_of_publication,
    'publisher': publisher

})

books_new

books_new = books_new[:20000]

books_new

"""**Data Preparation untuk Model Development dengan Collaborative Filtering**

Pertama, proses pengkodean fitur 'User-ID' dan 'ISBN' menjadi indeks bilangan bulat dilakukan. Terapkan kode berikut ini.

*mengonversi User-ID ke daftar tanpa nilai yang cocok*
"""

user_ids = df_rating['User-ID'].unique().tolist()
print('list userIDs: ', user_ids)

"""*melakukan pengkodean User-ID*"""

user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID: ', user_to_user_encoded)

"""*Melakukan proses pengkodean nomor ke dalam User-ID*"""

user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded number to userID: ', user_encoded_to_user)

"""Selanjutnya, lakukan proses yang sama untuk 'ISBN'.

*mengonversi ISBN ke daftar tanpa mencocokkan value*
"""

isbn_id = df_rating['ISBN'].unique().tolist()

"""*melakukan pengkodean ISBN*"""

isbn_to_isbn_encoded = {x: i for i, x in enumerate(isbn_id)}

"""*melakukan proses pengkodean nomor ke ISBN*"""

isbn_encoded_to_isbn = {i: x for i, x in enumerate(isbn_id)}

"""memetakan User-ID dan ISBN ke kerangka data terkait."""

pd.options.mode.chained_assignment = None

df_rating['user'] = df_rating['User-ID'].map(user_to_user_encoded)

df_rating['book_title'] = df_rating['ISBN'].map(isbn_to_isbn_encoded)

"""Periksa beberapa aspek data, seperti jumlah pengguna, jumlah judul buku, dan ubah nilai peringkat menjadi mengambang."""

num_users = len(user_to_user_encoded)
print(num_users)

num_book_title = len(isbn_to_isbn_encoded)
print(num_book_title)

df_rating['Book-Rating'] = df_rating['Book-Rating'].values.astype(np.float32)

min_rating = min(df_rating['Book-Rating'])

max_rating = max(df_rating['Book-Rating'])

print('Number of Users: {}, Number of Books: {}, Min Rating: {}, Max Rating: {}'.format(
     num_users, num_book_title, min_rating, max_rating
))

"""Tahap persiapan data telah selesai. Data sekarang siap untuk digunakan dalam proses pemisahan menjadi Data Trainning dan Data Validation dalam proses pengembangan collaborative filtering model.

# **Modeling**

**Pengembangan Model dengan Content-Based Filtering**
"""

data = books_new
data.sample(5)

"""TF-IDF Vectorizer"""

from sklearn.feature_extraction.text import TfidfVectorizer

tf = TfidfVectorizer()

tf.fit(data['book_author'])

tf.get_feature_names_out()

tfidf_matrix = tf.fit_transform(data['book_author'])

tfidf_matrix.shape

tfidf_matrix.todense()

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=data.book_title
).sample(15, axis=1).sample(10, axis=0)

"""Cosine Similarity"""

from sklearn.metrics.pairwise import cosine_similarity

"""*Menghitung cosine similarity pada matrix tf-idf*"""

cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

"""*Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa judul buku*"""

cosine_sim_df = pd.DataFrame(cosine_sim, index=data['book_title'], columns=data['book_title'])
print('Shape:', cosine_sim_df.shape)

"""*Melihat matriks kemiripan untuk setiap judul buku*"""

cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""Mendapatkan Rekomendasi"""

def book_recommendation(book_title, similarity_data=cosine_sim_df, items=data[['book_title', 'book_author']], k=5):

     index = similarity_data.loc[:,book_title].to_numpy().argpartition(range(-1, -k, -1))

     closest = similarity_data.columns[index[-1:-(k+2):-1]]

     closest = closest.drop(book_title, errors='ignore')

     return pd.DataFrame(closest).merge(items).head(k)

"""menghasilkan 5 rekomendasi buku teratas yang direkomendasikan oleh sistem."""

book_title_test = "DEVIL ON HORSEBACK" # book title example

data[data.book_title.eq(book_title_test)]

"""*Mendapatkan rekomendasi untuk judul buku yang serupa*"""

book_recommendation(book_title_test)

"""Berdasarkan output di atas, sistem berhasil merekomendasikan 5 judul buku teratas dengan kategori nama penulis buku ('Holt Victoria	').

**Model Development dengab Collaborative Filtering**

Memisahkan Data untuk Training dan Validation
"""

df_rating = df_rating.sample(frac=1, random_state=42)
df_rating

x = df_rating[['user', 'book_title']].values

y = df_rating['Book-Rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

train_indices = int(0.9 * df_rating.shape[0])
x_train, x_val, y_train, y_val = (
     x[:train_indices],
     x[train_indices:],
     y[:train_indices],
     y[train_indices:]
)

print(x, y)

"""Proses Training"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_book_title, embedding_size, dropout_rate=0.2, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_book_title = num_book_title
        self.embedding_size = embedding_size
        self.dropout_rate = dropout_rate

        self.user_embedding = layers.Embedding(
            num_users,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=tf.keras.regularizers.l2(1e-6)
        )
        self.user_bias = layers.Embedding(num_users, 1)

        self.book_title_embedding = layers.Embedding(
            num_book_title,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=tf.keras.regularizers.l2(1e-6)
        )
        self.book_title_bias = layers.Embedding(num_book_title, 1)

        self.dropout = layers.Dropout(rate=dropout_rate)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_vector = self.dropout(user_vector)
        user_bias = self.user_bias(inputs[:, 0])

        book_title_vector = self.book_title_embedding(inputs[:, 1])
        book_title_vector = self.dropout(book_title_vector)
        book_title_bias = self.book_title_bias(inputs[:, 1])

        dot_user_book_title = tf.tensordot(user_vector, book_title_vector, 2)

        x = dot_user_book_title + user_bias + book_title_bias

        return tf.nn.sigmoid(x)

model = RecommenderNet(num_users, num_book_title, 50)

model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=1e-4),
    metrics = [tf.keras.metrics.RootMeanSquaredError()]
)

"""*memulai proses training*"""

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 16,
    epochs = 50,
    validation_data = (x_val, y_val)
)

"""Mendapatkan Rekomendasi Judul Buku"""

book_df = books_new

user_id = df_rating['User-ID'].sample(1).iloc[0]
book_readed_by_user = df_rating[df_rating['User-ID'] == user_id]

book_not_readed = book_df[~book_df['isbn'].isin(book_readed_by_user['ISBN'].values)]['isbn']
book_not_readed = list(
    set(book_not_readed)
    .intersection(set(isbn_to_isbn_encoded.keys()))
)

book_not_readed = [[isbn_to_isbn_encoded.get(x)] for x in book_not_readed]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_readed), book_not_readed)
)

"""Untuk mendapatkan rekomendasi judul buku, gunakan model.predict() dari library Keras."""

ratings_model = model.predict(user_book_array).flatten()

top_ratings_indices = ratings_model.argsort()[-10:][::-1]

recommended_book_ids = [
    isbn_encoded_to_isbn.get(book_not_readed[x][0]) for x in top_ratings_indices
]

top_book_user = (
    book_readed_by_user.sort_values(
        by='Book-Rating',
        ascending=False
    )
    .head(10)['ISBN'].values
)

book_df_rows = book_df[book_df['isbn'].isin(top_book_user)]

"""*Menampilkan rekomendasi buku dalam bentuk DataFrame*"""

book_df_rows_data = []
for row in book_df_rows.itertuples():
    book_df_rows_data.append([row.book_title, row.book_author])

recommended_book = book_df[book_df['isbn'].isin(recommended_book_ids)]

recommended_book_data = []
for row in recommended_book.itertuples():
    recommended_book_data.append([row.book_title, row.book_author])

"""*Membuat DataFrame untuk output*"""

output_columns = ['Book Title', 'Book Author']
df_book_readed_by_user = pd.DataFrame(book_df_rows_data, columns=output_columns)
df_recommended_books = pd.DataFrame(recommended_book_data, columns=output_columns)

print("Showing recommendation for users: {}".format(user_id))
print("===" * 9)
print("Book with high ratings from user")
print("----" * 8)
print(df_book_readed_by_user)
print("----" * 8)
print("Top 10 books recommendation")
print("----" * 8)
df_recommended_books

"""Berdasarkan output di atas, rekomendasi telah berhasil diberikan kepada pengguna. Hasil di atas adalah rekomendasi untuk pengguna dengan id 2288. Dari output ini, dapat dilakukan perbandingan antara 'Buku dengan rating tinggi dari pengguna' dan 'Rekomendasi 10 buku teratas' untuk pengguna.

# **Evaluation:**

Model Evaluation dengan Content-Based Filtering
"""

threshold = 0.5

ground_truth = np.where(cosine_sim >= threshold, 1, 0)

ground_truth_df = pd.DataFrame(ground_truth, index=data['book_title'], columns=data['book_title']).sample(5, axis=1).sample(10, axis=0)

ground_truth_df

import numpy as np
from sklearn.metrics import precision_recall_fscore_support

sample_size = 1000

sub_indices = np.random.choice(cosine_sim.shape[0], sample_size, replace=False)
cosine_sim_sample = cosine_sim[sub_indices, :][:, sub_indices]
ground_truth_sample = ground_truth[sub_indices, :][:, sub_indices]

cosine_sim_flat = cosine_sim_sample.reshape(-1)
ground_truth_flat = ground_truth_sample.reshape(-1)

predictions = (cosine_sim_flat >= threshold).astype(int)
precision, recall, f1, _ = precision_recall_fscore_support(
    ground_truth_flat, predictions, average='binary', zero_division=1
)

print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)

"""**Model Evaluation dengan Collaborative Filtering**"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""Berdasarkan hasil visualisasi metrik evaluasi RMSE untuk model yang dikembangkan, terlihat bahwa model konvergen pada sekitar 50 epoch, dan berdasarkan plot metrik model memberikan nilai MSE yang relatif kecil. Dari proses ini, diperoleh nilai error akhir sebesar 0.2941, dan error pada data validasi sebesar 0.3355. Nilai-nilai ini menunjukkan hasil yang cukup baik untuk sistem rekomendasi yang dihasilkan. Semakin kecil nilai RMSE, maka semakin baik model tersebut dalam memprediksi preferensi pengguna terhadap suatu barang. Hal inilah yang membuat hasil rekomendasi dari model cukup akurat."""